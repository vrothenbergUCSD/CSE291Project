{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (0.1.8)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from ollama) (0.27.0)\n",
      "Requirement already satisfied: anyio in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.2.0)\n",
      "Requirement already satisfied: certifi in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.6)\n",
      "Requirement already satisfied: sniffio in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.1.16-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.29-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Downloading aiohttp-3.9.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
      "  Downloading langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
      "  Downloading langchain_core-0.1.45-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.50-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from langchain) (1.26.3)\n",
      "Collecting pydantic<3,>=1 (from langchain)\n",
      "  Downloading pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading multidict-6.0.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading yarl-1.9.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading marshmallow-3.21.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain) (23.2)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.1-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.18.2 (from pydantic<3,>=1->langchain)\n",
      "  Downloading pydantic_core-2.18.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.5-cp311-cp311-macosx_11_0_arm64.whl (390 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.2/390.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_community-0.0.34-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.1.45-py3-none-any.whl (291 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.3/291.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Downloading langsmith-0.1.50-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.18.2-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.29-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading orjson-3.10.1-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (250 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.6/250.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.9.4-cp311-cp311-macosx_11_0_arm64.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tenacity, SQLAlchemy, pydantic-core, orjson, mypy-extensions, multidict, marshmallow, jsonpatch, frozenlist, annotated-types, yarl, typing-inspect, pydantic, aiosignal, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
      "Successfully installed SQLAlchemy-2.0.29 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.6.0 dataclasses-json-0.6.4 frozenlist-1.4.1 jsonpatch-1.33 langchain-0.1.16 langchain-community-0.0.34 langchain-core-0.1.45 langchain-text-splitters-0.0.1 langsmith-0.1.50 marshmallow-3.21.1 multidict-6.0.5 mypy-extensions-1.0.0 orjson-3.10.1 pydantic-2.7.1 pydantic-core-2.18.2 tenacity-8.2.3 typing-inspect-0.9.0 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(\"how can langsmith help with testing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='white-space: pre-wrap; width:800px;'>\n",
       "Langsmith is a tool that can be used to help with testing in several ways:\n",
       "\n",
       "1. **Code Generation**: Langsmith can generate test cases based on the API documentation of a language model, which can save time and effort compared to writing tests manually.\n",
       "2. **Test Case Prioritization**: Langsmith can prioritize test cases based on their likelihood of detecting errors, which can help identify the most critical tests to run first.\n",
       "3. **Model Validation**: Langsmith can validate the correctness of a language model by generating test inputs and checking whether the model produces the expected outputs.\n",
       "4. **Test Data Generation**: Langsmith can generate test data automatically, which can be useful for testing models that require a large amount of training data.\n",
       "5. **Hyperparameter Tuning**: Langsmith can optimize hyperparameters of a language model using automated testing and evaluation, which can improve the model's performance.\n",
       "6. **Model Comparison**: Langsmith can compare the performance of different language models by generating test inputs and evaluating their outputs, which can help identify the best model for a particular task.\n",
       "7. **Explainability**: Langsmith can generate explanations for the predictions of a language model, which can help understand how the model works and identify any biases or errors.\n",
       "8. **Robustness Analysis**: Langsmith can analyze the robustness of a language model by generating test inputs that are designed to expose its limitations and vulnerabilities.\n",
       "9. **Security Testing**: Langsmith can identify potential security vulnerabilities in a language model by generating test cases that simulate attacks on the model, such as adversarial examples.\n",
       "10. **Compliance Checking**: Langsmith can check whether a language model complies with regulatory requirements or ethical guidelines by generating test cases that evaluate its performance against these standards.\n",
       "\n",
       "By using Langsmith to automate testing, developers can save time and effort while improving the quality and reliability of their language models.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(f\"<div style='white-space: pre-wrap; width:800px;'>{response}</div>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (0.2.64)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from llama-cpp-python) (4.9.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from llama-cpp-python) (1.26.3)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from llama-cpp-python) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vince/miniforge3/envs/cse251u/lib/python3.11/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /Users/vince/Work/llama.cpp/models/llama-3-8b-Instruct.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = llama-3-8b-Instruct\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 5.33 GiB (5.70 BPW) \n",
      "llm_load_print_meta: general.name     = llama-3-8b-Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   562.00 MiB, ( 1001.95 / 21845.34)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  5459.93 MiB\n",
      "llm_load_tensors:      Metal buffer size =   561.99 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Max\n",
      "ggml_metal_init: picking default device: Apple M1 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   248.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     8.00 MiB, ( 1010.95 / 21845.34)\n",
      "llama_kv_cache_init:      Metal KV buffer size =     8.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   164.02 MiB, ( 1174.97 / 21845.34)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\", 'tokenizer.ggml.eos_token_id': '128001', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.model': 'gpt2', 'llama.attention.head_count_kv': '8', 'llama.context_length': '8192', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '500000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '17', 'llama.feed_forward_length': '14336', 'llama.block_count': '32', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'llama-3-8b-Instruct', 'llama.vocab_size': '128256'}\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|end_of_text|>\n",
      "Using chat bos_token: <|begin_of_text|>\n",
      "ggml_metal_free: deallocating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm not sure what you're referring to.\"\n",
      "\"I think we're both aware of the situation,\" said the man, his eyes fixed on her.\n",
      "Sarah felt a shiver run down her spine as she realized that she was trapped.\n",
      "\n",
      "\"Please don't hurt me,\" she begged, trying to keep her voice steady.\n",
      "\n",
      "The man's expression didn't change. \"I'm not going to hurt you,\" he said, his voice low and even. \"But I do need your cooperation.\"\n",
      "\n",
      "Sarah felt a surge of fear as the man's words seemed to imply that she was in some kind of danger. She tried to keep her thoughts focused on finding a way out of this situation, but it was difficult to think clearly with the man's eyes fixed so intently on hers.\n",
      "\n",
      "\"I don't know what you want me to do,\" Sarah said, trying to sound brave and confident.\n",
      "\n",
      "The man didn't respond immediately. Instead, he seemed to be studying her face, as if he were searching for something specific.\n",
      "\n",
      "Sarah felt a shiver run down her spine as she realized that the man was probably going to try to kidnap her. She tried to think quickly of a way to escape from this situation, but it was difficult to think clearly with the man's eyes fixed so"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1857.27 ms\n",
      "llama_print_timings:      sample time =      71.29 ms /   256 runs   (    0.28 ms per token,  3590.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1857.25 ms /     2 tokens (  928.63 ms per token,     1.08 tokens per second)\n",
      "llama_print_timings:        eval time =   20223.04 ms /   255 runs   (   79.31 ms per token,    12.61 tokens per second)\n",
      "llama_print_timings:       total time =   23601.14 ms /   257 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' I\\'m not sure what you\\'re referring to.\"\\n\"I think we\\'re both aware of the situation,\" said the man, his eyes fixed on her.\\nSarah felt a shiver run down her spine as she realized that she was trapped.\\n\\n\"Please don\\'t hurt me,\" she begged, trying to keep her voice steady.\\n\\nThe man\\'s expression didn\\'t change. \"I\\'m not going to hurt you,\" he said, his voice low and even. \"But I do need your cooperation.\"\\n\\nSarah felt a surge of fear as the man\\'s words seemed to imply that she was in some kind of danger. She tried to keep her thoughts focused on finding a way out of this situation, but it was difficult to think clearly with the man\\'s eyes fixed so intently on hers.\\n\\n\"I don\\'t know what you want me to do,\" Sarah said, trying to sound brave and confident.\\n\\nThe man didn\\'t respond immediately. Instead, he seemed to be studying her face, as if he were searching for something specific.\\n\\nSarah felt a shiver run down her spine as she realized that the man was probably going to try to kidnap her. She tried to think quickly of a way to escape from this situation, but it was difficult to think clearly with the man\\'s eyes fixed so'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/vince/Work/llama.cpp/models/llama-3-8b-Instruct.Q5_K_M.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    n_batch=512,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "llm.invoke(\"Hello.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi! How can I help you today?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1857.27 ms\n",
      "llama_print_timings:      sample time =       2.92 ms /    10 runs   (    0.29 ms per token,  3423.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3201.19 ms /    24 tokens (  133.38 ms per token,     7.50 tokens per second)\n",
      "llama_print_timings:        eval time =     817.11 ms /     9 runs   (   90.79 ms per token,    11.01 tokens per second)\n",
      "llama_print_timings:       total time =    4075.93 ms /    33 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"[SYSTEM]You are an expert clinician tasked with analyzing notes from an Electronic Health Record system. Your will review the discharge notes for a patient and respond with \"\n",
    "user_input = cleaned_text\n",
    "full_prompt = f\"[SYSTEM]{system_prompt}\\n\\n[USER]: {user_input}\\n\\n[CLINICIAN]:\"\n",
    "\n",
    "response = llm.invoke(full_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032-DS-21</td>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853</td>\n",
       "      <td>2180-05-07</td>\n",
       "      <td>Subject ID: 10000032, HAdm ID: 22595853, Chart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032-DS-22</td>\n",
       "      <td>10000032</td>\n",
       "      <td>22841357</td>\n",
       "      <td>2180-06-27</td>\n",
       "      <td>Subject ID: 10000032, HAdm ID: 22841357, Chart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032-DS-23</td>\n",
       "      <td>10000032</td>\n",
       "      <td>29079034</td>\n",
       "      <td>2180-07-25</td>\n",
       "      <td>Subject ID: 10000032, HAdm ID: 29079034, Chart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000032-DS-24</td>\n",
       "      <td>10000032</td>\n",
       "      <td>25742920</td>\n",
       "      <td>2180-08-07</td>\n",
       "      <td>Subject ID: 10000032, HAdm ID: 25742920, Chart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000084-DS-17</td>\n",
       "      <td>10000084</td>\n",
       "      <td>23052089</td>\n",
       "      <td>2160-11-25</td>\n",
       "      <td>Subject ID: 10000084, HAdm ID: 23052089, Chart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15223</th>\n",
       "      <td>10486120-DS-3</td>\n",
       "      <td>10486120</td>\n",
       "      <td>21496368</td>\n",
       "      <td>2113-11-03</td>\n",
       "      <td>Subject ID: 10486120, HAdm ID: 21496368, Chart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15224</th>\n",
       "      <td>10486130-DS-15</td>\n",
       "      <td>10486130</td>\n",
       "      <td>25382870</td>\n",
       "      <td>2148-09-18</td>\n",
       "      <td>Subject ID: 10486130, HAdm ID: 25382870, Chart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15225</th>\n",
       "      <td>10486144-DS-25</td>\n",
       "      <td>10486144</td>\n",
       "      <td>21214054</td>\n",
       "      <td>2124-04-06</td>\n",
       "      <td>Subject ID: 10486144, HAdm ID: 21214054, Chart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15226</th>\n",
       "      <td>10486144-DS-26</td>\n",
       "      <td>10486144</td>\n",
       "      <td>28877892</td>\n",
       "      <td>2124-07-14</td>\n",
       "      <td>Subject ID: 10486144, HAdm ID: 28877892, Chart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15227</th>\n",
       "      <td>10486144-DS-27</td>\n",
       "      <td>10486144</td>\n",
       "      <td>20471114</td>\n",
       "      <td>2125-05-19</td>\n",
       "      <td>Subject ID: 10486144, HAdm ID: 20471114, Chart...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15228 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              note_id  subject_id   hadm_id   charttime  \\\n",
       "0      10000032-DS-21    10000032  22595853  2180-05-07   \n",
       "1      10000032-DS-22    10000032  22841357  2180-06-27   \n",
       "2      10000032-DS-23    10000032  29079034  2180-07-25   \n",
       "3      10000032-DS-24    10000032  25742920  2180-08-07   \n",
       "4      10000084-DS-17    10000084  23052089  2160-11-25   \n",
       "...               ...         ...       ...         ...   \n",
       "15223   10486120-DS-3    10486120  21496368  2113-11-03   \n",
       "15224  10486130-DS-15    10486130  25382870  2148-09-18   \n",
       "15225  10486144-DS-25    10486144  21214054  2124-04-06   \n",
       "15226  10486144-DS-26    10486144  28877892  2124-07-14   \n",
       "15227  10486144-DS-27    10486144  20471114  2125-05-19   \n",
       "\n",
       "                                                    text  \n",
       "0      Subject ID: 10000032, HAdm ID: 22595853, Chart...  \n",
       "1      Subject ID: 10000032, HAdm ID: 22841357, Chart...  \n",
       "2      Subject ID: 10000032, HAdm ID: 29079034, Chart...  \n",
       "3      Subject ID: 10000032, HAdm ID: 25742920, Chart...  \n",
       "4      Subject ID: 10000084, HAdm ID: 23052089, Chart...  \n",
       "...                                                  ...  \n",
       "15223  Subject ID: 10486120, HAdm ID: 21496368, Chart...  \n",
       "15224  Subject ID: 10486130, HAdm ID: 25382870, Chart...  \n",
       "15225  Subject ID: 10486144, HAdm ID: 21214054, Chart...  \n",
       "15226  Subject ID: 10486144, HAdm ID: 28877892, Chart...  \n",
       "15227  Subject ID: 10486144, HAdm ID: 20471114, Chart...  \n",
       "\n",
       "[15228 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/discharge_journal_df.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.iloc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject ID: 10000032, HAdm ID: 22595853, Chart Time: 2180-05-07 00:00:00;Name:  ___  Unit No:  ___;Admission Date:  ___  Discharge Date:  ___;Date of Birth:  ___  Sex:  F;Service: MEDICINE;Allergies: No Known Allergies / Adverse Drug Reactions;Attending: ___;Chief Complaint: Worsening ABD distension and pain;Major Surgical or Invasive Procedure: Paracentesis;History of Present Illness: ___ HCV cirrhosis c/b ascites, hiv on ART, h/o IVDU, COPD, bioplar, PTSD, presented from OSH ED with worsening abd distension over past week. Pt reports self-discontinuing lasix and spirnolactone ___ weeks ago, because she feels like \"they don't do anything\" and that she \"doesn't want to put more chemicals in her.\" She does not follow Na-restricted diets. In the past week, she notes that she has been having worsening abd distension and discomfort. She denies ___ edema, or SOB, or orthopnea. She denies f/c/n/v, d/c, dysuria. She had food poisoning a week ago from eating stale cake (n/v 20 min after food ingestion), which resolved the same day. She denies other recent illness or sick contacts. She notes that she has been noticing gum bleeding while brushing her teeth in recent weeks. she denies easy bruising, melena, BRBPR, hemetesis, hemoptysis, or hematuria. Because of her abd pain, she went to OSH ED and was transferred to ___ for further care. Per ED report, pt has brief period of confusion - she did not recall the ultrasound or bloodwork at osh. She denies recent drug use or alcohol use. She denies feeling confused, but reports that she is forgetful at times. In the ED, initial vitals were 98.4 70 106/63 16 97%RA Labs notable for ALT/AST/AP ___ ___: ___, Tbili1.6, WBC 5K, platelet 77, INR 1.6;Past Medical History: 1. HCV Cirrhosis 2. No history of abnormal Pap smears. 3. She had calcification in her breast, which was removed previously and per patient not, it was benign. 4. For HIV disease, she is being followed by Dr. ___ Dr. ___. 5. COPD 6. Past history of smoking. 7. She also had a skin lesion, which was biopsied and showed skin cancer per patient report and is scheduled for a complete removal of the skin lesion in ___ of this year. 8. She also had another lesion in her forehead with purple discoloration. It was biopsied to exclude the possibility of ___'s sarcoma, the results is pending. 9. A 15 mm hypoechoic lesion on her ultrasound on ___ and is being monitored by an MRI. 10. History of dysplasia of anus in ___. 11. Bipolar affective disorder, currently manic, mild, and PTSD.;12. History of cocaine and heroin use.;Social History: ___ Family History: She a total of five siblings, but she is not talking to most of them. She only has one brother that she is in;touch with and lives in ___. She is not aware of any known GI or liver disease in her family. Her last alcohol consumption was one drink two months ago. No regular alcohol consumption. Last drug use ___ years ago. She quit smoking a couple of years ago.;Physical Exam: VS: 98.1 107/61 78 18 97RA General: in NAD HEENT: CTAB, anicteric sclera, OP clear Neck: supple, no LAD CV: RRR,S1S2, no m/r/g Lungs: CTAb, prolonged expiratory phase, no w/r/r Abdomen: distended, mild diffuse tenderness, +flank dullness, cannot percuss liver/spleen edge ___ distension GU: no foley Ext: wwp, no c/e/e, + clubbing Neuro: AAO3, converse normally, able to recall 3 times after 5 minutes, CN II-XII intact;Discharge: PHYSICAL EXAMINATION: VS: 98 105/70 95 General: in NAD HEENT: anicteric sclera, OP clear Neck: supple, no LAD CV: RRR,S1S2, no m/r/g Lungs: CTAb, prolonged expiratory phase, no w/r/r Abdomen: distended but improved, TTP in RUQ, GU: no foley Ext: wwp, no c/e/e, + clubbing Neuro: AAO3,  CN II-XII intact;Pertinent Results: ___ 10:25PM  GLUCOSE-109* UREA N-25* CREAT-0.3* SODIUM-138 POTASSIUM-3.4 CHLORIDE-105 TOTAL CO2-27 ANION GAP-9 ___ 10:25PM  estGFR-Using this ___ 10:25PM  ALT(SGPT)-100* AST(SGOT)-114* ALK PHOS-114* TOT BILI-1.6* ___ 10:25PM  LIPASE-77* ___ 10:25PM  ALBUMIN-3.3* ___ 10:25PM  WBC-5.0# RBC-4.29 HGB-14.3 HCT-42.6 MCV-99* MCH-33.3* MCHC-33.5 RDW-15.7* ___ 10:25PM  NEUTS-70.3* LYMPHS-16.5* MONOS-8.1 EOS-4.2* BASOS-0.8 ___ 10:25PM  PLT COUNT-71* ___ 10:25PM  ___ PTT-30.9 ___ ___ 10:25PM  ___ .;CXR: No acute cardiopulmonary process. U/S: 1. Nodular appearance of the liver compatible with cirrhosis. Signs of portal hypertension including small amount of ascites and splenomegaly.;2. Cholelithiasis. 3. Patent portal veins with normal hepatopetal flow. Diagnostic para attempted in the ED, unsuccessful. On the floor, pt c/o abd distension and discomfort.;Brief Hospital Course: ___ HCV cirrhosis c/b ascites, hiv on ART, h/o IVDU, COPD, bioplar, PTSD, presented from OSH ED with worsening abd distension over past week and confusion.;# Ascites - p/w worsening abd distension and discomfort for last week. likely ___ portal HTN given underlying liver disease, though no ascitic fluid available on night of admission. No signs of heart failure noted on exam. This was ___ to med non-compliance and lack of diet restriction. SBP negative diuretics: > Furosemide 40 mg PO DAILY > Spironolactone 50 mg PO DAILY, chosen over the usual 100mg dose d/t K+ of 4.5. CXR was wnl, UA negative, Urine culture blood culture negative.;Pt was losing excess fluid appropriately with stable lytes on the above regimen. Pt was scheduled with current PCP for ___ check upon discharge. Pt was scheduled for new PCP with Dr. ___ at ___ and follow up in Liver clinic to schedule outpatient screening EGD and ___.;Medications on Admission: The Preadmission Medication list is accurate and complete. 1. Furosemide 20 mg PO DAILY 2. Spironolactone 50 mg PO DAILY 3. Albuterol Inhaler 2 PUFF IH Q4H:PRN wheezing, SOB 4. Raltegravir 400 mg PO BID 5. Emtricitabine-Tenofovir (Truvada) 1 TAB PO DAILY 6. Nicotine Patch 14 mg TD DAILY 7. Ipratropium Bromide Neb 1 NEB IH Q6H SOB;Discharge Medications: 1. Albuterol Inhaler 2 PUFF IH Q4H:PRN wheezing, SOB 2. Emtricitabine-Tenofovir (Truvada) 1 TAB PO DAILY 3. Furosemide 40 mg PO DAILY RX *furosemide 40 mg 1 tablet(s) by mouth Daily Disp #*30 Tablet Refills:*3 4. Ipratropium Bromide Neb 1 NEB IH Q6H SOB 5. Nicotine Patch 14 mg TD DAILY 6. Raltegravir 400 mg PO BID 7. Spironolactone 50 mg PO DAILY 8. Acetaminophen 500 mg PO Q6H:PRN pain;Discharge Disposition: Home;Discharge Diagnosis: Ascites from Portal HTN;Discharge Condition: Mental Status: Clear and coherent. Level of Consciousness: Alert and interactive. Activity Status: Ambulatory - Independent.;Discharge Instructions: Dear Ms. ___, It was a pleasure taking care of you! You came to us with stomach pain and worsening distension. While you were here we did a paracentesis to remove 1.5L of fluid from your belly. We also placed you on you 40 mg of Lasix and 50 mg of Aldactone to help you urinate the excess fluid still in your belly. As we discussed, everyone has a different dose of lasix required to make them urinate and it's likely that you weren't taking a high enough dose. Please take these medications daily to keep excess fluid off and eat a low salt diet. You will follow up with Dr. ___ in liver clinic and from there have your colonoscopy and EGD scheduled. Of course, we are always here if you need us. We wish you all the best! Your ___ Team.;Followup Instructions: ___ \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Replace consecutive newlines surrounded by optional whitespace with a single semicolon.\n",
    "    # This targets blocks of newlines that separate fields.\n",
    "    text = re.sub(r'(\\n[ \\t]*){2,}', ';', text)\n",
    "\n",
    "    # Remove remaining standalone newlines and leading/trailing whitespace around them\n",
    "    # This is for cases where a newline is in the middle of a field, e.g., \"Allergies: \\nNo Known Allergies\"\n",
    "    text = re.sub(r'[ \\t]*\\n[ \\t]*', ' ', text)\n",
    "\n",
    "    # Replace 2 or more consecutive spaces with exactly 2 spaces.\n",
    "    text = re.sub(r'[ \\t]{2,}', '  ', text)\n",
    "\n",
    "    # Remove any leading/trailing whitespace around semicolons to tidy up\n",
    "    text = re.sub(r'\\s*;\\s*', ';', text)\n",
    "\n",
    "    # Remove any unnecessary semicolons that appear immediately after colons\n",
    "    # which could happen in cases like \"Allergies:;No Known Allergies\"\n",
    "    text = re.sub(r':;', ': ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Example usage\n",
    "cleaned_text = clean_text(df.iloc[0].text)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /Users/vince/Work/llama.cpp/models/llama-3-8b-Instruct.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = llama-3-8b-Instruct\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 5.33 GiB (5.70 BPW) \n",
      "llm_load_print_meta: general.name     = llama-3-8b-Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   562.00 MiB, ( 2176.66 / 21845.34)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  5459.93 MiB\n",
      "llm_load_tensors:      Metal buffer size =   561.99 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Max\n",
      "ggml_metal_init: picking default device: Apple M1 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   496.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    16.00 MiB, ( 2192.66 / 21845.34)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    16.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   296.02 MiB, ( 2488.67 / 21845.34)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\", 'tokenizer.ggml.eos_token_id': '128001', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.model': 'gpt2', 'llama.attention.head_count_kv': '8', 'llama.context_length': '8192', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '500000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '17', 'llama.feed_forward_length': '14336', 'llama.block_count': '32', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'llama-3-8b-Instruct', 'llama.vocab_size': '128256'}\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|end_of_text|>\n",
      "Using chat bos_token: <|begin_of_text|>\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/vince/Work/llama.cpp/models/llama-3-8b-Instruct.Q5_K_M.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    n_batch=512,\n",
    "    n_ctx=4096,\n",
    "    # f16_kv=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# llm.invoke(\"Hello.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm happy to help you analyze these semi-structured notes on the expiratory phase!\n",
      "\n",
      "To start, let's take a closer look at the patient's condition:\n",
      "\n",
      "1. **Abdominal Distension**: The patient has worsening abdominal distension over the past week, likely due to portal hypertension given their underlying liver disease.\n",
      "2. **Portal Hypertension Signs**: The patient has signs of portal hypertension, including small amounts of ascites and splenomegaly.\n",
      "3. **Liver Disease**: The patient has cirrhosis (HCV-related) and cholelithiasis.\n",
      "\n",
      "Regarding the lab results:\n",
      "\n",
      "1. **Lab Values**:\n",
      "\t* Glucose: 109\n",
      "\t* Urea: 25\n",
      "\t* Creatinine: 0.3\n",
      "\t* Sodium: 138\n",
      "\t* Potassium: 3.4\n",
      "\t* Chloride: 105\n",
      "\t* Total CO2: 27\n",
      "\t* Anion Gap: 9\n",
      "\t* Est GFR: ______ (not specified)\n",
      "2. **Liver Enzymes**:\n",
      "\t* ALT (SGPT): 100*\n",
      "\t* AST (SGOT): 114*\n",
      "\t* Alk Phos: 114*\n",
      "\t* Total Bili: 1.6*\n",
      "3. **Blood Count**:\n",
      "\t* WBC: 5.0#\n",
      "\t* RBC: 4.29\n",
      "\t* HGB: 14.3\n",
      "\t* HCT: 42.6\n",
      "\t* MCV: 99*\n",
      "\t* MCH: 33.3*\n",
      "\t* MCHC: 33.5\n",
      "\t* RDW: 15.7*\n",
      "4. **Coagulation Studies**:\n",
      "\t* PTT: 30.9\n",
      "\n",
      "Now, let's summarize the patient's hospital course:\n",
      "\n",
      "1. **Admission Diagnosis**: Ascites from Portal HTN\n",
      "2. **Medications on Admission**: Furosemide (20 mg PO daily), Spironolactone (50 mg PO daily), Albuterol Inhaler (2 puffs IH Q4H PRN wheezing/SOB), Raltegravir (400 mg PO BID), Emtricitabine-Tenofovir (Truvada) 1 tab PO daily, Nicotine Patch (14 mg TD daily), Ipratropium Bromide Neb (1 neb IH Q6H SOB)\n",
      "3. **Discharge Medications**: Furosemide (40 mg PO daily), Emtricitabine-Tenofovir (Truvada) 1 tab PO daily, Albuterol Inhaler (2 puffs IH Q4H PRN wheezing/SOB), Ipratropium Bromide Neb (1 neb IH Q6H SOB), Nicotine Patch (14 mg TD daily), Raltegravir (400 mg PO BID), Spironolactone (50 mg PO daily), Acetaminophen (500 mg PO Q6H PRN pain)\n",
      "4. **Discharge Diagnosis**: Ascites from Portal HTN\n",
      "5. **Discharge Condition**: Mental Status: Clear and Coherent, Level of Consciousness: Alert and Interactive, Activity Status: Ambulatory - Independent\n",
      "\n",
      "Lastly, let's look at the discharge instructions:\n",
      "\n",
      "1. **Medication Regimen**: Take Furosemide (40 mg PO daily) and Spironolactone (50 mg PO daily) to help with excess fluid in the belly\n",
      "2. **Dietary Instructions**: Eat a low-salt diet to reduce fluid retention\n",
      "3. **Follow-up Appointments**: Schedule appointments with Dr. ___ in liver clinic for further evaluation and management\n",
      "\n",
      "What would you like me to focus on next?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "system_prompt = \"You are an expert clinical assistant, tasked with helping a user analyze semi-structured notes from an Electronic Health Record system.\"\n",
    "user_input = cleaned_text\n",
    "full_prompt = f\"[SYSTEM]:{system_prompt}\\n\\n[USER]: {user_input}\\n\\n[ASSISTANT]: \"\n",
    "\n",
    "response = llm.invoke(full_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient is a 58-year-old female who presented to the emergency department with worsening abdominal distension and confusion. The patient has a history of HCV cirrhosis, HIV on ART, IVDU, COPD, bioplar, PTSD, and was admitted from the OSH ED with worsening abdominal distension over the past week. The patient underwent a paracentesis to remove 1.5L of fluid from her belly and was placed on Lasix and Aldactone to help urinate the excess fluid still in her belly.\n",
      "\n",
      "The patient's lab results showed:\n",
      "\n",
      "* Glucose-109\n",
      "* Urea-25\n",
      "* Creatinine-0.3\n",
      "* Potassium-3.4\n",
      "* Chloride-105\n",
      "* Total CO2-27\n",
      "* Anion gap-9\n",
      "* Lipase-77\n",
      "* Albumin-3.3\n",
      "* WBC-5.0\n",
      "* RBC-4.29\n",
      "* HGB-14.3\n",
      "* HCT-42.6\n",
      "* MCV-99\n",
      "* MCH-33.3\n",
      "* MCHC-33.5\n",
      "* RDW-15.7\n",
      "\n",
      "The patient's imaging results showed:\n",
      "\n",
      "* No ascites fluid seen on CT scan\n",
      "\n",
      "The patient was discharged with instructions to take Lasix and Aldactone daily to keep excess fluid off and eat a low salt diet. The patient was also scheduled to follow up with Dr. ___ in liver clinic to schedule outpatient screening EGD and colonoscopy.\n",
      "\n",
      "The patient's medication list is accurate and complete, including Furosemide 20 mg PO DAILY, Spironolactone 50 mg PO DAILY, Albuterol Inhaler 2 PUFF IH Q4H:PRN wheezing, SOB, Emtricitabine-Tenofovir (Truvada) 1 TAB PO DAILY, Ipratropium Bromide Neb 1 NEB IH Q6H SOB, Raltegravir 400 mg PO BID, Acetaminophen 500 mg PO Q6H:PRN pain.\n",
      "\n",
      "The patient's discharge instructions are clear and concise, outlining the medications to be taken daily, the low salt diet to be followed, and the scheduled follow-up appointment with Dr. ___. The patient was also informed of the importance of keeping the appointments and following the medication regimen to manage their condition effectively.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\")\n",
    "\n",
    "system_prompt = \"You are an expert clinical analyst, tasked with summarizing semi-structured notes from an Electronic Health Record system. Summarize the notes in a concise manner.\"\n",
    "user_input = cleaned_text\n",
    "full_prompt = f\"[SYSTEM]:{system_prompt}\\n\\n[NOTES]: {user_input}\\n\\n[ANALYST]: \"\n",
    "\n",
    "response = llm.invoke(full_prompt)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse251u",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
